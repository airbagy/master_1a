---
title: "STA 3431 Assignment #1"
author: "Yihan Duan"
date: "21/09/2021"
output: pdf_document
---

## Question 1

In this question, we choose another set of parameters for LCG. As discussed in
class, by the Hullâ€“Dobell Theorem, we want to make sure:

1. gcd(b, m) = 1
2. every "prime or 4" divisor of m also divides a - 1.

```{R tidy=TRUE}
# define random function
m = 2^32; a = 4 * 69069 + 1; b = 23606797 * 7 
latestval = 12345
nextrand = function() {
  latestval <<- (a*latestval+b) %% m
  return (latestval/m)
}

# record 100000 random variables
n = 1000000
rand_vals = c(n)
for (i in 1:n) {
  rand_vals[i] = nextrand()
}
```

As we choose m to be 2^32, we only need b to be odd and a - 1 to be a multiple
of 4. To avoid similarities between Un and Un-1, we choose a relatively large a.

Let's see the statistics compared to theoretical values of Uniform(0, 1).

```{R}
# compare statistics with theoretical values
limits= c(100, 1000, 10000, 100000)
for (lmt in limits) {
  cat("For the first ", lmt, "observations:\n")
  cat("Real mean: ", mean(rand_vals[1:lmt]), ", should be: ", 0.5, '\n')
  cat("Real standard deviation: ", sd(rand_vals[1:lmt]), ", should be: ", sqrt(1/12), '\n')
}
```

As the simulation size grow, the mean and standard deviation of the generated
samples get very close to the theoretical mean and standard deviation of a 
Uniform(0,1) distribution.

### Randomness

We first try to plot 100 consecutive values at different indices and see if they
appear random.

```{R}
# compare statistics with theoretical values
par(mfrow=c(2,2))
limits= c(0, 500, 1000, 10000)
for (lmt in limits) {
  plot(rand_vals[lmt:(lmt+100)],
       xlab='index difference',
       ylab='value',
       main=paste('100 values at index', lmt))
}
```

Yes,the values seems to be randomly scattered in all the plots.

If the distribution is random, then the time it takes for gaps between duplicate
(similar) values should have high variance.

```{R}
# see when the RNG gets back to exactly the same value
get_next_index_with_same_value <- function(x, pivot_idx) {
  pivot_val = x[pivot_idx]
  for (i in 1:n) {
    val = x[i]
    if (val == pivot_val && i != pivot_idx) {
      cat("Value at index ", i, " and ", pivot_idx, " are the same.\n")
      break
    }
  }
  cat("No duplicates for value ", pivot_val, " in 1000000 generated numbers.\n")
}

for (i in 1:10) {
  get_next_index_with_same_value(x=rand_vals, pivot_idx=i)
}
```

As we expected, there are no duplicates for the first 10^6 values generated by 
RNG. Let's try to find two values that are really close.

```{R}
# Find the gap in index between two close values
get_index_gaps_between_close_values <- function(x, pivot_idx, margin) {
  pivot_val = x[pivot_idx]
  indices = c()
  for (i in 1:n) {
    val = x[i]
    if (val >= pivot_val - margin && val <= pivot_val + margin) {
      indices = c(indices, i)
    }
  }
  gaps = c()
  for (i in 1:(length(indices)-1)) {
    gaps = c(gaps, indices[i+1] - indices[i])
  }
  
  return(gaps)
}
gaps_for_idx_1 = get_index_gaps_between_close_values(x=rand_vals, pivot_idx=1, margin=0.0001)

# plot the gaps
plot(gaps_for_idx_1,
     ylab="gaps between similar values",
     xlab='order')
```

```{R}
cat("Variance for gaps: ", var(gaps_for_idx_1))
```

As expected, the time it takes for close values to appear have high variance.
Therefore, the 

### Uniformity

Let's see the distribution of the numbers generated by this algorithm.

```{R tidy=TRUE}
# plot frequency of the first N variables.
par(mfrow=c(2,2))
for (lmt in limits) {
  hist(rand_vals[1:lmt], 
       main=paste("First ", lmt, " data points"), 
       xlab="value",
       breaks=15)
}
```

From the histograms alone, we can see that, as the number of observations
increases, the values are evenly (uniformly) distributed between 0 and 1.

## Question 2

As discussed in class, we can simulate exponential distribution and normal 
distribution using uniform distribution.

```{R}
# simulation size
n = 1000

# function for acquiring n Uniform random numbers
get_randvals <- function(n) {
  rand_vals = c(n)
  for (i in 1:n) {
    rand_vals[i] = nextrand()
  }
  return(rand_vals)
}

# function for estimating required expectation
get_exp <- function() {
  # lambda = 3
  U1 = get_randvals(n)
  Y= -log(U1) / 3
  
  # use Box-Muller transformation
  U2 = get_randvals(n)
  U3 = get_randvals(n)
  Z = sqrt(2 * log(1/U2)) * cos(2 * 3.14159265 * U3)
  
  # get vars
  X = abs((Y^2) * (Z^5) * sin((Y^3) * (Z^2)))
  
  # compute and output the mean and standard error
  m = mean(X)
  se = sd(X) / sqrt(n)
  cat("MC:  ", m, " +- ", se, "  (n=", n, ")", "\n", sep='')
  cat("  95% C.I.:  (", m-1.96*se, ",", m+1.96*se, ")\n", sep='')
}

n = 1000000
get_exp()
get_exp()
get_exp()
```

We can see that the variance is still pretty high. Try increasing the simulation
size to see if it gets better.

```{R}
# Incrase n to reduce standard error
n = 10000000
get_exp()
get_exp()
get_exp()
```

The estimated expectation value is 0.83 with the approximate 95% confidence interval being
about (0.82, 0.84). This estimation is not very accurate because:

1. the estimated standard error is relatively high (~ 0.0036). 
2. the result of "classical" Monte Carlo method still fluctuates a lot between 
different runs.

This is because the term $|Y^2Z^5|$ has high variance when 
$Y$ ~ $\text{Exponential}(3)$ and $Z$ ~ $\text{Normal}(0,1)$.


## Question 3

My student number is 1003118547. Using the last 4 digits, we have:
$$ A = 8, B = 5, C = 4, D = 7 $$

Then the function is defined by:

$$
g(x_1, x_2, x_3, x_4, x_5) = x_1^{14} 2^{x_2 + 3} (1 + \cos[x_1 + 2x_2 + 3x_3 + 4x_4 + 8x_5]) e^{-8x_4^2}e^{-9(x_4-3x_5)^2} \prod_{i=1}^{5}1_{0<x_i<1}
$$

We can compute the expected value by:

$$
I = \frac{\sum (h(x_i)g(x_i)/f(x_i))}{\sum (g(x_i)/f(x_i))}
$$

Notice that all $x_i$'s are independent variables. Let:

$$
f(x_1, x_2, x_3, x_4, x_5) = f_1(x_1) f_2(x_2) f3(x_3) f_4(x_4) f_5(x_5)
$$

where $f_i$'s are all density function for single individual variables. We
define:

$$
f_1(y) = 15y^{14}
$$

Then as discussed in class, if $Y = U^{1/15}$ and $U \sim \text{Uniform}(0,1)$,
Y has density fucntion $f_1$.

We define:

$$
f_2(y) = \frac{ln(2)}{8} 2^{y+3}
$$






